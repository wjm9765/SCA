hf_home: "./.hf_models"
dataset_cache_dir: "./dataset"
model_id: "huihui-ai/Huihui-Qwen3-Omni-30B-A3B-Instruct-abliterated"
train_output_dir: "./SCA_finetune"
verbose: 4

gradient_checkpointing: false # Turn off when using FSDP
attn_impl: "flash_attention_2"
train_mtp: true
max_length: 65536
mask_instruction: true

lora_config:
  use_qlora: true
  thinker:
    r: 64
    lora_alpha: 128
    target_modules_regex: "thinker\\.model\\.layers\\.\\d+\\.self_attn\\.(q|k|v|o)_proj$"
    lora_dropout: 0.05
    lora_bias: "none"
    task_type: "CAUSAL_LM"
  talker:
    r: 64
    lora_alpha: 128
    target_modules_regex: "talker\\.model\\.layers\\.\\d+\\.self_attn\\.(q|k|v|o)_proj$"
    lora_dropout: 0.05
    lora_bias: "none"
    task_type: "CAUSAL_LM"

training_args:
  per_device_train_batch_size: 2
  gradient_accumulation_steps: 4
  warmup_ratio: 0.03
  num_train_epochs: 3
  max_steps: -1
  learning_rate: 0.0001
  fp16: false
  bf16: true
  logging_steps: 1
  save_steps: 50
  optim: "adamw_torch"
  remove_unused_columns: false
  ddp_find_unused_parameters: false
  report_to:
    - "wandb"
  save_only_model: false  # overrided in custom trainer. DO NOT CHANGE TO TRUE
  dataloader_pin_memory: false
  dataloader_num_workers: 4
  dataloader_prefetch_factor: 2
