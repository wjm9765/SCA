hf_home: "./.hf_models"
dataset_cache_dir: "./dataset"
model_id: "huihui-ai/Huihui-Qwen3-Omni-30B-A3B-Instruct-abliterated"
train_output_dir: "./SCA_finetune"
verbose: 4

gradient_checkpointing: false # Turn off when using FSDP
attn_impl: "flash_attention_2"
train_mtp: true
max_length: 65536
mask_instruction: true

lora_config:
  use_qlora: true
  thinker:
    r: 32
    use_dora: false
    lora_alpha: 64
    # NOTE: MLP projections (gate|up|down_proj) don't match in MoE models like Qwen3-Omni.
    # MoE uses expert-based naming (e.g., mlp.experts.X.gate_proj). Only attention LoRA is applied.
    target_modules_regex: "thinker\\.model\\.layers\\.\\d+\\.(self_attn\\.(q|k|v|o)_proj|mlp\\.(gate|up|down)_proj)$"
    lora_dropout: 0.05
    lora_bias: "none"
    task_type: "CAUSAL_LM"
  talker:
    r: 32
    use_dora: false
    lora_alpha: 64
    # NOTE: Talker LoRA currently not applied - code_predictor is trained via modules_to_save instead.
    # MLP projections also don't match MoE naming.
    target_modules_regex: "talker\\.model\\.layers\\.\\d+\\.(self_attn\\.(q|k|v|o)_proj|mlp\\.(gate|up|down)_proj)$"
    lora_dropout: 0.05
    lora_bias: "none"
    task_type: "CAUSAL_LM"

training_args:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 2
  warmup_ratio: 0.05
  num_train_epochs: 3
  max_steps: -1
  learning_rate: 0.0001
  max_grad_norm: 1.0
  fp16: false
  bf16: true
  logging_steps: 1
  save_steps: 100
  optim: "adamw_torch"
  remove_unused_columns: false
  ddp_find_unused_parameters: false
  report_to:
    - "wandb"
  save_only_model: false  # overrided in custom trainer. DO NOT CHANGE TO TRUE
  dataloader_pin_memory: true
  dataloader_num_workers: 4
  dataloader_prefetch_factor: 2
