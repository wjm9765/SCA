# Full Duplex Training Configuration
# This config is for training Qwen3-Omni on interleaved audio-text data

hf_home: "./.hf_models"
model_id: "huihui-ai/Huihui-Qwen3-Omni-30B-A3B-Instruct-abliterated"
train_output_dir: "./SCA_duplex_finetune"
verbose: 4

# Model settings
gradient_checkpointing: true  # Enable gradient checkpointing for FSDP
attn_impl: "flash_attention_2"
train_mtp: true
mtp_weight: 2.0
max_length: 32768

# Duplex-specific token IDs
silence_token_id: 151646
audio_token_id: 151675
pad_token_id: 151643

# Duplex training settings
max_segments_per_batch: 8

# LoRA configuration
lora_config:
  use_qlora: true
  use_dora: false
  r: 16
  lora_alpha: 16
  target_modules_regex: ".*(thinker|talker)\\.model\\.layers\\.\\d+\\.self_attn\\.(q|k|v|o)_proj$"
  lora_dropout: 0.05
  lora_bias: "none"
  task_type: "CAUSAL_LM"
  modules_to_save:
    - "talker.code_predictor"
    - "speaker_projection"

# Training arguments
training_args:
  per_device_train_batch_size: 1
  gradient_accumulation_steps: 1
  warmup_ratio: 0.05
  num_train_epochs: 10
  max_steps: -1
  learning_rate: 0.00001
  max_grad_norm: 1.0
  fp16: false
  bf16: true
  logging_steps: 1
  save_steps: 50
  optim: "adamw_torch"
  remove_unused_columns: false
  ddp_find_unused_parameters: false
  report_to:
    - "wandb"
  save_only_model: false
  dataloader_pin_memory: true
  dataloader_num_workers: 4
  dataloader_prefetch_factor: 2
